# 课后总结

这两部分课程囊括了关于计算机硬件和软件许多方向的知识。
我认为丰富还不是它最大的特点。
**最重要的是，它帮我建立起了一个非常棒的知识体系**。
这个清晰、简洁又坚实的骨架结构，
可以支撑我不断去深入学习有关计算机任何一个方向的具体内容。

如今的网络虽然让信息变得丰富又触手可及，
但是却因为缺少像这门课程所做的知识结构梳理，
所以总让人觉得越学越糊涂，记住的东西越多离真相越远。

所以我的课后总结侧重在回忆出这副”骨架“，
而且在总结的过程中，我发现自己仍然在轻松的惬意地，向这个体系里不断添加着新的知识。

## 计算机的硬件构造

一台计算机，不论有多么先进，都是由是四大类硬件组成的。

### 核心组件，数据存储和芯片

1. 数据存储设备

   如，ROM、RAM
   
   - ROM（read-only memory，只读），用来加载程序指令
   - RAM（random-access memory，可乱序存取），用来存取程序执行中所产生的数据

   本课程里的ROM和RAM是同一种硬件，即硬件结构、原理一样。由一大串Register，寄存器组成。

   <details>
   <summary>数据存储设备还有其他类型</summary>
    
   - DADS（direct-access data storage media，有序存储）
   
     如，CD，DVD，HDD，磁带。

     此类设备的存储地址有远近区别。
	 而RAM的读取可以认为是等距的，读取任何位置所需要的时间和复杂度相同。
   </details>
   
2. 芯片

   由一个ALU、几个Register和一个Program Counter构成一个核心单元。
   本课程的芯片是单核CPU，而更先进的芯片可能包含了多个核心单元，或有更强的计算功能ALU。
  
   - ALU（arithmetic and logic unit），数学和逻辑运算单元，由若干种逻辑门搭建。

   - Register，寄存器，一般由8/16/32/64个bit构成。一个bit能存一位数，0或者1。

     这里的“8/16/32/64”描述的是CPU“线宽”，对应“8位、16位、32位、64位”机。
     
   - Program Counter，指令地址寄存器/程序计数器，由寄存器，步进器，和两个控制信号组成。
    
### 外设，输入和输出

3. 输入设备
  
   向RAM指定地址推送数据
  
4. 输出设备

   从RAM指定地址取用数据

输入和输出设备使用RAM哪段地址，推送的是什么样的数据，
这些数据应该被怎样解读，多久刷新一次数据……
有关电脑与其他设备之间的分工配合问题，都是靠硬件协议和驱动程序来规范的。

输入和输出设备也可能有芯片，有存储。可以是一台能独立运行的设备，如另一台电脑。
但要承担的职责无外乎往指定地址推送数据，或是从指定地址取用数据。

核心组件是必不可少的，
没有芯片就不能做任何运算，没有存储就没有地方加载程序，和储存运算过程中产生的数据。
但是外设理论上讲不是必须要有。
  
## 内存（RAM）的结构

### RAM的基础元素，寄存器
 
RAM的基础单元是Register，寄存器。

和CPU里的寄存器结构一样，线宽一般也是一样的。同时也是每次进行数据读取的最小单位。
   
一个寄存器由n个bit组成，一个bit由一个Mux加一个DFF（data flip-flop）组成，存储1位数据：0或1。

<img alt=“bit数字逻辑图” src=/SVG/bit.svg>

这样的一个结构的目的是，
在寄存器没有接到任何指令时，
让它在每个时钟周期结束时重新刷新一下自己的数据，
以防止过长时间没有新数据输入，当前存储的数据（电荷）流失。
如果接到“加载”指令，load=1，那就把输入端的数据加载进来，
如果load=0，那么输入端数据怎么变化都不会影响寄存器里的数据。
   
### RAM的结构，实现任意位置等效读取
   
由不超过2<sup>n</sup>个寄存器组成的RAM，
可以仅用一个同样结构的寄存器来完成选址工作。

找到并存取RAM中任何一个位置的数据，难度和用时都是一样的。

本课程的寄存器是16位的，
但是由于程序指令的最后一位被设计用来区分A或C类指令，
只能用剩下的15位来做地址选择，所以最大的选址范围是2的15次方，
一次可以对32768个寄存器进行等效读取。
   
<img alt=“15位register选址结构图” src=/SVG/RAM.svg>
   
## 芯片（CPU）的结构

### CPU=ALU+2*Register+PC

本课程的芯片是一个CPU（central processing unit），中央处理器。
由1个ALU，2个Register和1个Program Counter构成。

 - ALU（arithmetic and logic unit），数学和逻辑运算单元 

   ALU的构成：
   - 16位全加器 Full Adder（两个半加器Half Adder构成）
   - 16位Mux、16位Not、16位And等

   ALU的结构和所实现的计算：
   - 输入2个16位数据，6个1位的控制信号

     2个16位数据：x(16), y(16);<br>
     6个控制信号：zero x, not x, zero y, not y, + or &, not out; <br>

     这6个控制信号对应的计算步骤是顺序发生的，<br>
     也就是说，先处理x，再处理y，然后对x和y进行计算，再对结果进行一次处理。
     
   - 输出1个16位数据，2个1位的控制信号

     输出数据：out(16)<br>
     控制信号：zero, negative<br>
     
   - 靠6个控制信号可以对输入数据进行的计算是

     直接得到：0, 1, -1, x, y, !x, !y, -x, -y, <br>
     计算：x+1, x-1, y+1, y-1, x+y, x-y, y-x, x&y, x|y

   <img alt=“ALU数字逻辑电路图” src=/SVG/ALU.svg>
          
 - Register，寄存器
   
   - D Register

     存储数据。由C类指令指挥ALU计算并输出结果到这个寄存器里。
     
   - A Register

     接收指令，程序指令如果是A类指令，会直接读到这个寄存器里；<br>
     作为RAM选址器，当需要输出或读取M（即，RAM[A]）时。
     
 - PC（Program Counter），指令地址寄存器/程序计数器。
  
  PC的主要构成：
   - 16位增量器/步进器 Incrementer
   - 1位的reset控制信号
   - 1位的load控制信号

  PC是ROM的选址器，可以被手动归零（由reset控制），
  没有指令时每读一行程序（一个时钟周期）会自+1，
  也可以由C类指令控制将A寄存器的数据输入到PC里。
     
### CPU的结构

 - D寄存器 作为ALU的一个输入（x），
 - A寄存器/RAM[A] 作为ALU的另一个输入（y），
 - A寄存器可以输出到PC，
 - ALU可以输出到D寄存器，A寄存器，M（RAM[A]）里，
   可以输出到这三个中的任意一个或多个位置，也可以不输出。
 - PC 作为ROM的选址器，会把ROM[PC]输入A寄存器（如果是A类指令），
 - 或用做整个CPU里不同位置的控制信号（如果是C类指令）
   - 第15位决定指令是A类还是C类；
   - 第12位决定向ALU里输入A还是M；
   - 第6-11位决定ALU进行什么计算；
   - 第3-5位决定ALU结果输出到什么地方；
   - 第0-2位决定是否输出A到PC；
   - 第13-14位没用，C类指令时一般写1。
   
<img alt=“CPU数字逻辑电路图” src=/SVG/CPU.svg>
    
## 芯片做什么事情？

### 只处理二进制数据

CPU只能接收二进制数据，进行二进制下的数学运算和逻辑运算，输出的也是二进制数据。
（原因：芯片和数据存储设备的硬件结构限制）

二进制加减法和逻辑运算是两种毫不相干的运算。
但是在把逻辑规则中的互相对立结论用0和1来表达时，
逻辑运算和二进制加减法就有了神奇的重合部分。
   
比如，1111 1111 1111 1111减去任何一个16位二进制数x[16]，
得到的结果与对x[16]取非（not x[16]）是一样的。
由于二进制个位数只有0和1，进位是1。
所以不论做什么样的运算，每一位上的数无非就是在0和1两者之间变换。
所以我们是在寻找二进制下的运算结果和逻辑运算结果能重合的规律，
找到这些规律就能找到实现二进制运算的逻辑门组合。

所以当我们让CPU帮我们做二进制运算时，CPU实际上只是在做逻辑运算。

### 接收数据输入，一次最多一个
   
一个16位线宽的CPU，一次最多能从CPU的外部，
也就是数据存储设备（比如ROM、RAM）里，
获得一个16位的二进制数据，

经过一个时钟周期的计算后，可以得到一个16位的二进制的计算结果，

然后在下一个周期，可以把这个结果输入回CPU里进行下一步的运算，
或者输出到CPU的外部，写到RAM里。

虽然运算单元ALU每次都可以接收两个输入，
但这两个输入里只有一个是接收来自CPU外部的数据，
另一个则是以前步骤的计算结果暂存到D然后输回给ALU的。

从CPU外部获取数据有两种来源/方式，
一是在ROM读出PC指定的地址里的数据，输入A然后再输入ALU；
另一种是读出RAM里A表示的地址里的数据，然后直接输出给ALU。

### 运算，输出计算结果

不论是取自程序指令，还是内存，或是前几步的计算结果，
一旦把数据给到ALU之后，它就开始运算。

ALU能做什么样的运算，就意味着CPU能做什么样的运算。

本课程的ALU有一个16位全加器，它能让x和y进行**加减运算**，
或者让x、y中任意一个和0、1之间做加减运算；

另外ALU中的16位与、或、非门，可以支持x和y之间做**与，或，非**的逻辑运算，
以及x、y之中任意一个和0、1之间做与、或、非的逻辑运算。

 ### 顺序执行程序，或者跳转

输入什么数据，做什么样的运算，都是由程序指令来控制的。

程序指令加载在ROM里，CPU会从第一条指令，也就是从ROM[0]的位置开始，一条接一条读取。
或者说，加载在ROM里的程序会被一条接一条送进CPU里。
如果没接到跳转指令，那么就会从ROM的第一个寄存器一直读到最后一个寄存器。
遇到A类指令，就把这条指令本身放到A寄存器里，
遇到C类指令，这条指令就只会被用做控制信号。

控制信号会决定让两个输入数据做哪种运算，
会控制计算结果输出到什么地方，
还会决定下一步读哪条指令。

读ROM哪个位置，也就是读第几条指令，其实是由PC，指令地址寄存器，控制的，
我们把PC的硬件设计成启动计算机时PC是0，所以CPU总是会从ROM[0]开始读程序指令。

然后每读一次（一个时钟周期后）PC自+1，
所以一个时钟周期结束，在没有接到特别指令时，下一条就自动被送进CPU。

如果读到跳转指令：将A写入到了PC里，
那么下一个时钟周期，ROM[A]位置的指令会被送入CPU，
接下来就会从A这个位置往后继续一条一条把指令送入CPU。
如果reset被按下，会让PC归零，然后又开始从ROM[0]的位置往CPU送指令。
   
## 我们如何指挥“芯片”？

知道了芯片能做的事情，
接下来就是如何指挥芯片做我们需要的事，
以及我们到底能指挥芯片做什么样的事了。

现如今一个电脑能做的事情非常多，令人惊叹。
显然我们从输入一个二进制数据，然后计算两个二进制数据，得到一个新的二进制数据，
发展出了极其复杂的一套东西让芯片来执行。
能指挥芯片做什么，这个问题已经被人类诠释的丰富的不能再丰富了，
而且似乎延展的可能性仍然是无穷的。

我们是如何指挥芯片完成了如此复杂的工作呢？答案只有两个字：**编程**。

编程语言非常多种多样，我们是这样给他们分类的：
更符合人类语言和思维习惯的一般被称之为（翻译成）高级语言，
芯片直接能认识的叫机器码，接近机器执行逻辑的语言叫低级语言/汇编语言

### 芯片能接受的指令：机器语言/机器码

机器能阅读的程序指令就是一行一行和芯片线宽相同位数的二进制数。
这些二进制数可以被分成两大指令类型。

 - A类指令，以0开头（第15位是0）
 
   0vvv vvvv vvvv vvvv。
   读到这类指令就是告诉芯片，现在向A寄存器里存入刚读到的这个0开头的二进制数字。
   
 - C类指令，以1开头（第15位是1）
 
   111a cccc ccdd djjj。
   这类指令不会被输入到寄存器或ALU，
   而是会被加载到整个CPU内部各个地方的控制信号路线上。
   遇到这类指令
   就是告诉芯片现在按照各个控制信号线路加载的信号去决定运算、输出、和程序跳转。
   - 第6-11位决定ALU进行什么计算；cccccc
   - 第12位决定向ALU里输入A还是M；a
   - 第3-5位决定ALU结果输出到什么地方；ddd
   - 第0-2位决定是否输出A到PC；jjj
   - 第13-14位没用，一般写1。

每种芯片的ALU能进行的运算可以是不一样的。
所以每种芯片都有自己对应的机器码和汇编语言。

### 底层语言/低级语言：汇编语言

比0101 1101 0101 0001更适合人阅读的，最接近机器语言的一种编程语言。
用符号来表示不同的机器指令，
用这类语言编程与用机器码编程几乎没有区别，相当于是直接指挥芯片。

芯片能做的事情有：
 - 接下来要从外部的什么位置输入一个数据，或不从外部输入数据：

   将ROM里的数输入A：@A <br>
   从RAM[A]取数：M <br>
   就用A里现有的数：A
   
 - 决定ALU做什么样的运算：

   加减法：D+A，M-D，M+1，D-1 <br>
   逻辑运算：D｜M，D&A，！M <br>
   直接输出某个输入数据：D，M，A，0，1，-1
     
 - 把计算结果输出到哪儿：

   哪儿都不输出：null <br>
   一个地方：D=（运算）<br>
   多个地方：AMD=（运算），AD=（运算）
   
 - 跳转到某条程序（可以附加一个条件，满足才跳转）：

   @A<br>
   condition（运算）; jump

   如果condition满足jump条件就跳转（PC=A），
   不满足就继续执行下一条程序（PC++）。
   如果写成：condition; JMP，就是无条件跳转。

以上四类符号指令与机器指令是逐一对应的，
借助汇编器就可以把些符号一条对一条地转换成二进制的机器码。

另外借助汇编器还可以比较轻松的实现标签功能。
一类标签是对RAM常用地址进行命名，
这样需要取用这些地址的数据，比写@A，D=M这类语句的时候，指向更清晰一些。
汇编器翻译时，只需要建立一个常用RAM地址名标签表，
把所有@地址命标签的，都替换就可以。
另一类标签是标记程序执行过程中的跳入跳出点，
跳入和跳出都加同样名称的标签，不同的格式。由汇编器在翻译机器码时，
根据标签名称，就可以在跳出指令中写入正确的跳入位置（指令的序号）。

### 中间语言和虚拟机

如何让，一个只能将两个数据进行运算的芯片和两串顺序排列的寄存器，
帮我们实现复杂的工作呢？

比如：
- 重复特定的计算步骤
- 接受三个或者更多的输入（变量），
如果部分变量有多个共享使用者，
如果需要输入的是其他计算步骤的计算结果
- 计算过程产生了需要暂存的中间结果且不止一个，
或者中间结果要间隔很多计算步骤以后才会被用到
- 某个问题计算到一半，想要调用其他固定计算步骤，
或是插入一个临时的不相干的工作
- 怎么样识别、保存和使用一串彼此关联的数据

我们为内存设计了一套使用方法来解决以上的问题。

包括：
- 将内存切分成几个功能区域segment（R0-R15/temp/...），
来分别存储指针数据、不同类型的变量（local/argument/static/）、
计算过程，输入输出数据
- 建立指针pointer（SP/LCL/ARG/THIS/THAT/SCREEN/KBD），
用来记录内存区域的划分，以便能准确找到特定区域里的数据
- 动态使用栈stack，工作展开时（调用公式），
输入变量、过程要用到的变量会被放到栈上，
然后随着公式的推进，中间过程和最终结果也会跟着写在栈上，
工作完成后把结果返回给公式的调用者，并清空这个工作进程所占用的区域。
如果需要在中间插入新工作（中间调用新公式），那么就在插入位置以同样规则展开新工作。
如此，巧妙地实现了多个工作交叉进行且独立保存各自进度
- 堆heap分配，为关联数据和对象数据划拨出所需要的存储空间。
堆分配机制包括记录所有没用被使用的空间，
从未使用的部分中切割出来需要的空间并把地址反馈给需求方，
回收不再被使用的空间，将回收的零碎空间重新整理成一个完整的大空间

这套精巧的内存使用方法同时也是所有高级编程语言共有功能的基础：
- 定义和调用公式（function/method/construction）
- 创建和导入类class、对象object
- 定义和使用某些实际是是数据串的数据类型（array/list/string...）
- 打包和执行程序（本质上是归类和调用一组公式）
- 文件存取（本质上是定义和存取数据串）

设立中间语言和虚拟机环节并不是必须，
但是它可以专注实现这套内存使用方法，还能承担跨芯片平台编译机器码的工作，
从而大大减轻高级语言的编译器设计负担。
具体来说就是由中间语言向不同芯片的汇编语言进行翻译，
这样芯片结构改变，汇编语言有变化，而中间语言没有变化时，
不用调整高级语言到中间语言的编译器，仅需调整虚拟机翻译器就可以。

### 从高级语言到机器码

高级编程语言往往更接近于人类语言（英文词汇和符号），容易理解。
只要会一些英语，在学习了程序语法之后，
就可以通过这些英文词汇和符号的组合，了解它要实现的事情。
而机器语言（指：0101 0011 0110 1001），
即便是精通芯片结构规则的人，也很难将它们和具体的指令一一对应起来。

语言每“高级”一层，就更容易被读懂，语法也随之复杂一些。
如果把机器码的语法复杂度设定为0，人类语言的语法复杂度是9，
汇编语言的复杂度大概是1，中间语言大概是2，早期的高级语言复杂度可能是3。
语法的复杂度主要体现在：
每个词汇的性质和含义是不是固定，是不是总会随着它前后词汇的不同，
或是它出现在语句种的位置不同而改变；
一句话是不是需要上下文来确定它的含义。

人们编程用高级语言，芯片工作只接受机器码的指挥。
我们是怎样用英文指挥芯片的呢？
就像让不同母语的人之间能交流，
从一种语言到另一种语言的转换总是通过翻译来实现。
我们和芯片对话也要经过从高级语言到机器码的翻译过程。
从高级语言到中间语言的翻译靠编译器complier，
从中间语言到汇编语言靠虚拟机翻译器VM translator，
从汇编语言到机器码靠汇编器assembler。

在学习和开发翻译器的过程中不难发现，
越是简单的语法（死板的语言），翻译时越容易做到准确和高效，
因为你不用反复理解和揣摩上下文。
编程语言的语法特别死板，那些大括号，分号，空格要求的存在，
就是在降低语法的复杂度。
在让人容易阅读的同时，减少向机器语言翻译的难度，
正是高级语言在设计时要寻找的平衡。
新生的编程语言和工具还在继续探索如何让编程进一步向人类语言靠拢。

机器语言对芯片来说是直接和准确的，
如果我们只用01来编写程序，那么我们给机器做的每一条指令都会被精准的执行。
但人类实在没办法适应纯用符号来进行思考。
我们需要借助自己熟悉的语言和工具。
因此，就像人类的语言向另一种语言翻译时都会遇到麻烦，
比如一句话存在有多种可能的翻译选项，选哪个更好？
或者是某些语句在另一个语言里缺少准确的对应，
只能通过扩写，或者舍弃部分含义，来取得近似的理解。
我们从高级语言向机器码编译的过程中，会出现不同的编译思路，
也会出现不精准的翻译，冗余的步骤，未知的错误，和其他各式各样的问题。

我们在用高级语言编程时写的是英文。
但实际上大部分词汇本身对于计算机来说却并不重要，
只要这些英文和符号的排列符合编程语法的规定它就能接受。
在学习过系统（OS）以后，对这个问题就更容易理解，
我把它留作最后一个话题。

## 操作系统、软件和程序语言、编程之间的关系

### 编程是做什么事情？编程语言又做了什么？

如果一个问题经常出现，每次都写一遍解决步骤就显得没有必要。
于是我们会把比较经典的解决步骤做成一个公式。
在解决一个大问题时，有些步骤需要被重复若干次，或者反复被使用，
那么也可以把它们做成一个公式。公式太多不方便查找，
于是我们进一步将同类型的，或是互相关联的公式整理成一类。

有时候程序要解决的是一些纯运算问题，如做乘除开方等复杂的数学运算。
有时候要构建一些东西，然后对这些东西做修改调整。
比如建立一个表格，然后写入或改变表格内的数据。
又比如在屏幕某个位置画了一个方块，
然后放大缩小它，改变它的颜色，或是上下左右挪动它。
这些东西都有各自的特征。
比如表格的行数列数和每个格里的数据；方块的边长，位置和颜色。
同时每一个具体物件有属于它的独立的数据，
比如一个表是3x3，另一个是5x2；我们在屏幕上画了10个大小不同的方快。
有时候我们会画很多个方块，但是只调整某一个方块的大小位置。
然而不论调整哪个方块，做放大、缩小、和改变位置的操作步骤又几乎是一样的。
我们为这类问题，设计了“面向对象”的编程语言架构。
和某个对象相关的公式也会被归为一类。
除此之外，这种类还要考虑实现创建对象，存储和调用特征数据等问题。

如果你知道有一个人已经写好了一套关于表格的公式，而且它很完美。
现在你的程序里正好需要构建一个表格。
那么你就不一定想要自己费很大力气，
从零开始写一大段还需要反复debug的新建表格代码。
所以我们把一些完美解决方案做成库。
这样在任何一个人编写新软件时，
都可以导入已经由别人写好的，并经过验证的完美公式。
高级语言都有若干基础库，和持续增加的扩展库。
我们通常在高级语言里考虑扩展功能的事情。
这种语言的开放性体现在，
不论扩展多少功能库、类，都不需要改写编译器complier。

要知道芯片能做的事情始终只是
进行逻辑判断，实现跳转步骤，对指定位置提取或存入数据，
以及做非常基础的数学运算。
我们巧妙地将这几个基础功能进行组合，就可以实现复杂一些的事情。
每一条指令，都是上一层（低级）语言的若干条指令的集合。
我们把常用的组合固定下来，并给他命名，让它成为编程语言的一条基础指令。
学习高级语言的一大块工作是在熟悉它提供了多少指令。
而编程，则主要是根据要实现的事情，分析并挑选出适合的指令，
然后把它们再组合成一个更大的结构的过程。
对已有公式的熟悉程度，对组合利用公式的方案的熟悉程度，
非常影响一个人编程的效率。

所有编程语言依存的逻辑几乎都是一样的。
给编程人员提供一套编写语法，调用格式，
并自有一套编译器来完成向芯片语言的翻译工作。
每个语句代表了一个别人写的公式，
而大部分公式只是这个语言的其他公式的集合。
如果觉得用某个语言写程序特别方便，要写的东西不多，
那可能只是因为它提供的库特别丰富。
语句被翻译，意味着把每个公式背后的代码集合展开。
当看起来简单的几条语句被完全展开时，
可能会是一个意想不到的庞大的代码数量。

### 操作系统和软件，即相似又不同

无论是写基础库，扩展库，写软件，还是写操作系统，
我们编写代码的工作都可以被描述为，定义类，然后在类里创建公式。
计算机的世界里只有如何识别类.公式和串接他们的语句，
我们预先告诉过它应该如何理解每种语句（编译）。
通过类.公式，它就知道如何能找到包含着被调用公式的那个文件，
然后将这个公式具体的执行方法复制过来（编译公式调用）。
它不知道自己打开的文件、找到的公式，
是属于一个软件的还是属于一个系统的，
只有编程的人知道自己在写的是系统还是软件。

那系统和软件是怎样随着按下开机键和点击图标就自动运行起来的呢？
我觉得就靠两个字“商量”，或者“协议”。
对于本课程这套简单的只执行一个程序的
计算机以及为它设计的操作系统和编程语言来说，
有以下四条协议：

- 硬件协议

	开机或重启键按下，计算机一定会从固定的位置，如ROM[0]，
	开始依次向后执行程序指令。

- 虚拟机协议

	虚拟机会完成分配内存区域和启动系统的工作。
	严格说是由虚拟机翻译器，
	在第一个执行程序的前面ROM[0]的位置添加booting code。
	分配指针位置，如sp=256，以及启动系统程序，
	如call Sys.init。

- 高级语言协议

	每个软件程序的启动，就是调用一个固定的公式，如Main.main()。
	因此在编写程序的时候，
	要把能引导整个软件启动的语句写在这个公式里。

- 操作系统协议

	系统启动程序（Sys.init）
	负责把系统的各个功能部分（OS class）
	需要的准备工作执行一遍。
	然后结束前调用程序的启动公式（call Main.main()），
	让软件启动起来。

有了以上这四条协议，
在规定语法，设计各级编译器，编写软件、系统时，
也包括在考虑硬件结构时，
就能清楚地知道如何界定功能，如何规范编写，如何进行分工配合。
如此保证不论写什么程序，
它经过complier，VM translator，
再转成机器码，导入ROM里以后，
一按下开机键，就会自动启动系统并执行这个程序。

这四种协议是不是唯一的方案呢？
显然不是。
首先它适用的场景是当前这台简单的计算机。
当计算机的功能变得复杂，要执行的程序不再是单一程序，
这四条协议的内容就可能不合适了。
即便是同样一台计算机，是否非要经过三层翻译，
booting code是不是只能在中间语言里靠虚拟机翻译器添加？
也显然不是这样，总是有其他协议方案的。
但一种协商结果，应该是综合起来看最舒适最合理的。
让整个计算机系统能顺畅运行的一个特点，就是商量，
商量出一套沟通合作办法后，各方都必须按协议办事。
输入输出设备有驱动，有规范，网络数据传输有协议，
想来如IEEE协会这种组织的存在，都是为方便大家进行“商量”的。

系统OS也是一堆类class，这让我感到意外。
其次没有想到的是，任何一个高级语言理论上都可以写系统。
只要同时用这个语言写一套能翻译成机器码的编译器，
那么用它写软件并运行在这个系统基础上是可行的，
理论上。

写操作系统要考虑的问题是很特别的，
是编写其他软件时几乎不太会考虑的方向。
系统提供了所有其他软件都几乎必然会用到的功能。
比如这个简单计算机的操作系统只有8个类：
- 纯运算类

  1 数学计算
  2 内存管理
  3 输出图像
  4 输出文字
  5 识别键盘输入
- 对象类

  6 字符串
  7 数列
	
- 相当于系统的Main class

  8 系统

如果展开看这8个类，
发现它们真的是写任何一个简单程序都离不开的公式，
哪怕是输出hello world这种入门语句（一定会用到输出文字，字符串）。
当然这些基础公式之间也避免不了相互引用，
比如输出文字需要用到输出图像和字符串。
字符串和数列要用到内存管理……
因为是向所有程序提供的基础功能，
所以几乎相当于所有软件的底层代码，
意味着每条公式都可能会被重复执行很多次，
所以我们最关心这些公式是否足够简洁（算法里说的时间复杂度和空间复杂度）。
以及我们是不是为所有搭建在这上面的复杂程序，找到了最合理的最小功能模块。

另外多数计算机不是将程序和系统在外部编译成机器码并导入一个ROM来使用，
而是把系统、程序、编译器都存在硬盘里，
在运行过程中随需要取出并进行编译工作。
这就很考验开发语言的编译器是否能写的比较简洁。
每翻译一条高级程序语句到底需要动用多少代码量，占用多少内存空间。
这样看，更接近人类说话习惯的编程语言就没有什么优势，
因为语法复杂度高，导致编译时不得不来回阅读上下文，
增加了不少工作量。
所以所有高级语言都可以写操作系统也只是一个理论上的说法。
现实中只有那几个语言被用来开发系统是有道理的。

软件是不让你看见类、公式如何命名如何归类以及代码如何编写的库。
系统和库是一个看得见公式、类的名称，甚至每个公式代码的软件。

## 计算机理解人类语言么？

### 单词是怎么被存储的？ASCII

我们给每个英文字母一个编号，区分大小写，所以a-z和A-Z是不同的数字。
每个字母对应一个整数。常见的符号也都有自己对应的数字编码。
整数可以被准确转换成二进制数字，
可以以一个固定长度的01码形式存在于内存、芯片、硬盘等地方。
当我们用键盘输入的时候，
按下一个按键，
键盘的输出端就会向内存的指定位置写入这个按键对应的ASCII码的二进制形式。
键盘的硬件一定要这样设计。

一个单词就是一串数字的有序集合array。
在这门课里，创建数列array是一个由系统OS提供功能，一个对象类class。

我们创建一个字符串string，它是一个array，
一行文字line，也是一个array。
一篇文章，可能是一个包含了一堆string和line的array。
这些文字在输入的人眼里是语言，
到了电脑那里不过是一串数字，最终是一串二进制编码。

我不知道英文26个字母为何被排列成A-Z，元辅音混排。
作为语言的基本符号，这种顺序和它的使用频率之间没什么联系，
数字的顺序隐含了大小关系，
我们按这个顺序给这些字母编了号，但这种逻辑和词汇语句没什么关联；
我们给大小写字母做了分组，却没有为元音和辅音做区分；
所以一个词汇被转换成数字以后，成了一套忽大忽小的数字组合，
看起来毫无道理。

为了让电脑服务更多语言，
我们有了更多类似于ASCII的字母字符与数字的对应表，
中文有GBxxxx。

### 怎么在屏幕上打出文字来？Font

在一个6x9的格子里把一些格涂黑，让涂黑的部分看起来像是字母A。<br>
比如：<br>
▢▢▩▩▢▢ 00001100<br>
▢▩▩▩▩▢ 00011110<br>
▢▩▢▢▩▢ 00010010<br>
▢▩▢▢▩▢ 00010010<br>
▩▩▢▢▩▩ 00110011<br>
▩▩▩▩▩▩ 00111111<br>
▩▩▢▢▩▩ 00110011<br>
▩▩▢▢▩▩ 00110011<br>
▩▩▢▢▩▩ 00110011<br>

对于本课程这种老式的512x256黑白屏电脑，
如果我想在屏幕左上角显示A，
可以就按照上面这个图，把屏幕上对应的像素点输出成黑色：
将内存中用于屏幕显示区域的
第0/32/64/96/128.../256这9个Register里的数，
改成00001100..0/00011110..0/00010010..0/00010010..0/00110011..0/
..../00110011..0，
屏幕上就会显示出一个A。
这就是输出字母的基本逻辑。

我们给每个字母都画这样一副像素图，
按涂色的格写1不涂色的写0的原则，
把每一行像素转换成一个二进制数，
如此每个字母都可以得到一个和像素图一致的数列。
再把这些数列以每个字母的ASCII码为索引，建立起一个查询表。
接下来我想让电脑显示哪个字母，
就可以很轻松地找到属于这个字母的像素图数列，
并把它显示在我需要的位置了。

我们安装的各式各样的字体文件Font，
储存的就是设计者为每个字符绘制的这样一套像素图案。
电脑屏幕变成了彩色输出，文字显示也有不同大小的字号，
但总的来说输出逻辑一样，
只是在这个基础图案上做一些简单的换算罢了。

### 孤独感让人类产生幻觉

存于电脑里的信息，说是一套密码一点都不为过，
因为你打开任意一个Register看到的就是一串二进制数字。
你需要一个正确的解码手册才能将他翻译成可以理解的信息。

比如这串数字（66,31,51,51,51,31,51,51,51,31,0,0），

 - 如果你认为它是一串ASCII码
 
   那么它将被翻译成B 333 333  ，夹杂着非打印字符的一串信息，
   
   有些莫名其妙。

 - 你也可以认为它只是一组数字，
  
   可是这组数字看起来实在没什么规律。

 - 这串数字其实记录的是字母B的像素图。

诚然我们进行解码的工作甚至是让电脑自己来完成的，
无需我们拿着一本解码手册来查询每个存储单元里的数字是什么含义。
而且电脑做这个工作的时候，我们还可以要求它把翻译的结果直接投影在屏幕上。
但如果你不觉得自家打印机打印一篇文章的过程是一种思考，
就不该认为电脑在屏幕上画出一些文字是一种智能的体现。
如果你不会觉得一张纸在被人画了副精美的图案之后，就拥有了绘者的智慧，
那么我们给电脑里存多少array，也无法让它拥有灵魂。

# End

课程结束 2024.3.5 

总结完成 2024.5.5
